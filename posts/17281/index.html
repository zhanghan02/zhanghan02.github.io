<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>解析Transformer模型 | ZhangHan个人博客</title><meta name="author" content="Zhang Han,944211286@qq.com"><meta name="copyright" content="Zhang Han"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="AI摘要：在 Transformer 模型中，Transformer 模型采用了自注意力机制，这是模型的主要组成部分之一。自注意力机制是模型的自我理解和记忆的能力，可以有效地捕捉到文本中的重要信息和关系，进一步提高了模型的性能。然而，虽然自注意力机制在很多方面都很强大，但它也有一些缺点。本文旨在探讨自注意力机制的一些缺点及其解决方案。  进入TransformerRNN很难处理冗长的文本序列，且很">
<meta property="og:type" content="article">
<meta property="og:title" content="解析Transformer模型">
<meta property="og:url" content="https://www.zhanghan.xyz/posts/17281/index.html">
<meta property="og:site_name" content="ZhangHan个人博客">
<meta property="og:description" content="AI摘要：在 Transformer 模型中，Transformer 模型采用了自注意力机制，这是模型的主要组成部分之一。自注意力机制是模型的自我理解和记忆的能力，可以有效地捕捉到文本中的重要信息和关系，进一步提高了模型的性能。然而，虽然自注意力机制在很多方面都很强大，但它也有一些缺点。本文旨在探讨自注意力机制的一些缺点及其解决方案。  进入TransformerRNN很难处理冗长的文本序列，且很">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/12/26/gab76QJTiDO2BkC.png">
<meta property="article:published_time" content="2023-12-26T01:46:22.000Z">
<meta property="article:modified_time" content="2024-04-24T10:54:46.873Z">
<meta property="article:author" content="Zhang Han">
<meta property="article:tag" content="ChatGPT">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/12/26/gab76QJTiDO2BkC.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.zhanghan.xyz/posts/17281/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?88f0b6eab573586585614e3e0b8047c9";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '解析Transformer模型',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-24 18:54:46'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper/swiper-bundle.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/bb/showbb_in_index.css"><script src="https://cdn.staticaly.com/gh/haonan15/CDN@main/source/waterfall.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2023/10/06/4pKqh38Gnk9bHC2.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">41</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/mylike/"><i class="fa-fw fas fa-link"></i><span> 我的喜欢</span></a></div><div class="menus_item"><a class="site-page" href="/offer/"><i class="fa-fw fas fa-book-open"></i><span> 面经</span></a></div><div class="menus_item"><a class="site-page" href="/ReadingNotes/"><i class="fa-fw fas fa-link"></i><span> ReadingNotes</span></a></div><div class="menus_item"><a class="site-page" href="/ChatGPT/"><i class="fa-fw fas fa-robot"></i><span> ChatGPT</span></a></div><div class="menus_item"><a class="site-page" href="/AIagent/"><i class="fa-fw fas fa-wallet"></i><span> AI agent</span></a></div><div class="menus_item"><a class="site-page" href="/%E9%9A%90%E7%A7%81%E6%94%BF%E7%AD%96/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2023/10/06/SJ9tkRVgloc134Z.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="ZhangHan个人博客"><span class="site-name">ZhangHan个人博客</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/mylike/"><i class="fa-fw fas fa-link"></i><span> 我的喜欢</span></a></div><div class="menus_item"><a class="site-page" href="/offer/"><i class="fa-fw fas fa-book-open"></i><span> 面经</span></a></div><div class="menus_item"><a class="site-page" href="/ReadingNotes/"><i class="fa-fw fas fa-link"></i><span> ReadingNotes</span></a></div><div class="menus_item"><a class="site-page" href="/ChatGPT/"><i class="fa-fw fas fa-robot"></i><span> ChatGPT</span></a></div><div class="menus_item"><a class="site-page" href="/AIagent/"><i class="fa-fw fas fa-wallet"></i><span> AI agent</span></a></div><div class="menus_item"><a class="site-page" href="/%E9%9A%90%E7%A7%81%E6%94%BF%E7%AD%96/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">解析Transformer模型</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-26T01:46:22.000Z" title="发表于 2023-12-26 09:46:22">2023-12-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-24T10:54:46.873Z" title="更新于 2024-04-24 18:54:46">2024-04-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%88%91%E7%9A%84GAI%E4%B9%8B%E8%B7%AF/">我的GAI之路</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>16分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="解析Transformer模型"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><div class="tip info"><p>AI摘要：在 Transformer 模型中，Transformer 模型采用了自注意力机制，这是模型的主要组成部分之一。自注意力机制是模型的自我理解和记忆的能力，可以有效地捕捉到文本中的重要信息和关系，进一步提高了模型的性能。然而，虽然自注意力机制在很多方面都很强大，但它也有一些缺点。本文旨在探讨自注意力机制的一些缺点及其解决方案。</p>
</div>
<h2 id="进入Transformer"><a href="#进入Transformer" class="headerlink" title="进入Transformer"></a>进入Transformer</h2><p>RNN很难处理冗长的文本序列，且很容易受到所谓梯度消失/爆炸的问题。RNN是按顺序处理单词的，所以很难并行化。</p>
<blockquote>
<p>用一句话总结Transformer：当一个扩展性极佳的模型和一个巨大的数据集邂逅，结果可能会让你大吃一惊。</p>
</blockquote>
<h2 id="Transformer是如何工作的？"><a href="#Transformer是如何工作的？" class="headerlink" title="Transformer是如何工作的？"></a>Transformer是如何工作的？</h2><h3 id="1-位置编码（Positional-Encodings）"><a href="#1-位置编码（Positional-Encodings）" class="headerlink" title="1.位置编码（Positional Encodings）"></a>1.位置编码（Positional Encodings）</h3><p>RNN中按照顺序处理单词，很难并行化。</p>
<p>位置编码的思路：将输入序列中的所有单词后面加上一个数字，表明它的顺序。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&quot;Dala&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;say&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;hello&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;world&quot;</span>,<span class="number">1</span>)]</span><br></pre></td></tr></table></figure>
<p>原文中使用正弦函数进行位置编码</p>
<p><strong>要点：将语序存储作为数据，而不是靠网络结构，这样你的神经网络就更容易训练了。</strong></p>
<h3 id="2-注意力机制（Attention）"><a href="#2-注意力机制（Attention）" class="headerlink" title="2.注意力机制（Attention）"></a>2.注意力机制（Attention）</h3><p>注意力是一种机制，它允许文本模型在决定如何翻译输出句子中的单词时“查看”原始句子中的每个单词。</p>
<p><img src="https://s2.loli.net/2023/12/26/gab76QJTiDO2BkC.png" alt=""></p>
<p>图源：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">[1409.0473] Neural Machine Translation by Jointly Learning to Align and Translate (arxiv.org)</a></p>
<p>模型如何知道在每个时间步长中应该注意哪些单词：就是从训练数据中学到的东西，通过观察成千上万的法语和英语句子，学会了什么类型的单词是相互依赖的。</p>
<h3 id="3-自注意力机制（Self-Attention）"><a href="#3-自注意力机制（Self-Attention）" class="headerlink" title="3.自注意力机制（Self-Attention）"></a>3.自注意力机制（Self-Attention）</h3><p>如何不是试图翻译单词，而是建立一个理解语言中的基本含义和模式的模型——一种可以用来做任何数量的语言任务的模型，怎么办？</p>
<p><strong>自注意力帮助神经网络消除单词歧义，做词性标注，命名实体识别，学习语义角色等等。</strong></p>
<hr>
<p>以上仅为读‍‍<a target="_blank" rel="noopener" href="https://waytoagi.feishu.cn/wiki/WCOcwp3DYiNj2mkiGVycjJ0Znaf">解析 Transformer 模型：理解 GPT-3、BERT 和 T5 背后的模型-飞书云文档 (feishu.cn)</a></p>
<p>的笔记，以下是我对Transformer进行的详细理解和技术解析。</p>
<h1 id="Transformer技术解析"><a href="#Transformer技术解析" class="headerlink" title="Transformer技术解析"></a>Transformer技术解析</h1><p><img src="https://s2.loli.net/2023/12/26/9RFyHObVKDijasT.png" alt=""></p>
<h2 id="1-位置编码Positional-Encoding"><a href="#1-位置编码Positional-Encoding" class="headerlink" title="1.位置编码Positional Encoding"></a>1.位置编码Positional Encoding</h2><p>从上面的整体框架图可以看出，Transformer中的Encoder和Decoder都是通过堆叠多头自注意力和全连接层得到的，不管是Encoder还是Decoder的输入，都进行了Positional Encoding，对于位置编码的作用，在原文是这样解释的：</p>
<blockquote>
<p>Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed</p>
</blockquote>
<p>由于我们的模型不包含递归和卷积，为了使模型能够利用序列的顺序，我们必须在序列中注入一些关于token的相对或绝对位置的信息。为此，我们在编码器和解码器堆栈底部的输入嵌入中加入位置编码。位置编码与embeddings具有相同的维数d，因此可以将两者求和。位置编码有多种选择，有学习的，也有固定的。</p>
<p><strong>Positional Encoding就是句子中词语相对位置的编码，让Transformer保留词语的位置信息。</strong></p>
<p>理想状态下，编码方式应该要满足以下几个条件，</p>
<ul>
<li><strong>对于每个位置的词语，它都能提供一个独一无二的编码</strong></li>
<li><strong>词语之间的间隔对于不同长度的句子来说，含义应该是一致的</strong></li>
<li><strong>能够随意延伸到任意长度的句子</strong></li>
</ul>
<p>以下是Transformer中使用的位置编码的公式：</p>
<p><img src="https://s2.loli.net/2023/12/26/lsvdkopVNnrPJFj.png" alt=""></p>
<p>PE将正弦函数用于偶数嵌入索引i，余弦函数用于奇数嵌入索引i。</p>
<p><img src="https://s2.loli.net/2023/12/26/Z2tQWT9PoXpmLnb.png" alt=""></p>
<p><img src="https://s2.loli.net/2023/12/26/PEaQAysUBtIeL6h.png" alt=""></p>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()     </span><br><span class="line">				<span class="comment"># 初始化位置编码矩阵  </span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">				<span class="comment"># 计算位置编码</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">				<span class="comment"># 将位置编码矩阵转换为张量</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#pe.requires_grad = False</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x + self.pe[:x.size(<span class="number">0</span>), :]</span><br></pre></td></tr></table></figure>
<p>这里再引申一下为什么位置嵌入会有用：</p>
<p><img src="https://s2.loli.net/2023/12/26/WwfyznlcOHZ41xT.png" alt=""></p>
<p>得出一个结论：$PE~pos+k~$可以被$PE~pos~$线性表示。从这一点来说，位置编码是可以反应一定的相对位置信息。</p>
<p>但是这种相对位置信息会在注意力机制那里消失。</p>
<h2 id="2-多头注意力机制"><a href="#2-多头注意力机制" class="headerlink" title="2.多头注意力机制"></a>2.多头注意力机制</h2><p><strong>为什么Transformer 需要进行 Multi-head Attention？</strong></p>
<p>最终的原因可以通过两句话来概括：</p>
<p>①为了解决模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身位置的问题；</p>
<p>②一定程度上ℎ越大整个模型的表达能力越强，越能提高模型对于注意力权重的合理分配。</p>
<h3 id="self-Attention自注意力机制"><a href="#self-Attention自注意力机制" class="headerlink" title="self-Attention自注意力机制"></a>self-Attention自注意力机制</h3><p><strong>所谓自注意力机制就是通过某种运算来直接计算得到句子在编码过程中每个位置上的注意力权重；然后再以权重和的形式来计算得到整个句子的隐含向量表示</strong>。最终，Transformer架构就是基于这种的自注意力机制而构建的Encoder-Decoder模型。</p>
<p><img src="https://s2.loli.net/2023/12/26/EWCuV2KeX15kDjr.png" alt=""></p>
<p>从上图可以看出，自注意力机制的核心过程就是<strong>通过Q和K计算得到注意力权重；然后再作用于V得到整个权重和输出</strong>。具体的，对于输入Q、K和V来说，其输出向量的计算公式为：</p>
<script type="math/tex; mode=display">
\\\begin{align*}q^{i} &= W^{q} \cdot a^{i} \\k^{i} &= W^{k} \cdot a^{i} \\v^{i} &= W^{v} \cdot a^{i} \\\alpha_{i,j} &= q^{i} \cdot k^{j} \\A &= \text{softmax}\left(\frac{QK^{T}}{\sqrt{d_k}}\right) \\\text{Attention}(Q, K, V) &= AV\\\end{align*}</script><ol>
<li>首先，对于输入序列中的每个元素，我们计算查询向量（Query）、键向量（Key）和值向量（Value），其中，Wq、Wk和Wv是需要学习的参数矩阵，ai是输入序列的第i个元素。</li>
<li>然后，我们利用查询向量和键向量来计算注意力得分（Attention Score），这里，αi,j表示第i个元素和第j个元素之间的注意力得分。</li>
<li>接着，我们对注意力得分进行缩放处理并应用softmax函数，以得到注意力权重。其中，Q和K分别是所有查询向量和键向量构成的矩阵，dk是键向量的维度。</li>
<li>最后，我们利用注意力权重和值向量来计算自注意力机制的输出。其中，V是所有值向量构成的矩阵，A是注意力权重矩阵。</li>
</ol>
<p><img src="https://s2.loli.net/2023/12/26/eI8rBG2luzbqd5v.png" alt=""></p>
<p>通过这种自注意力机制的方式确实解决了“传统序列模型在编码过程中都需顺序进行的弊端”的问题，有了自注意力机制后，仅仅只需要对原始输入进行几次矩阵变换便能够得到最终包含有不同位置注意力信息的编码向量。</p>
<h3 id="MultiHeadAttention"><a href="#MultiHeadAttention" class="headerlink" title="MultiHeadAttention"></a>MultiHeadAttention</h3><p>模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置，因此提出了通过多头注意力机制来解决这一问题。同时，使用多头注意力机制还能够给予注意力层的输出包含有不同子空间中的编码表示信息，从而增强模型的表达能力。</p>
<p><img src="https://s2.loli.net/2023/12/26/mghjZTXNxvEaQL9.png" alt=""></p>
<p>所谓的多头注意力机制其实就是将原始的输入序列进行多组的自注意力处理过程；然后再将每一组自注意力的结果拼接起来进行一次线性变换得到最终的输出结果。</p>
<script type="math/tex; mode=display">
\begin{align*}Q_i &= W_i^Q \cdot X \\K_i &= W_i^K \cdot X \\V_i &= W_i^V \cdot X \\\alpha_{i,j} &= \frac{Q_i \cdot K_j}{\sqrt{d_k}} \\A_i &= \text{softmax}(\alpha_{i,j}) \\\text{head}_i &= A_i \cdot V_i \\\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, ..., \text{head}_H) \cdot W^O\end{align*}</script><h3 id="为什么要使用多头"><a href="#为什么要使用多头" class="headerlink" title="为什么要使用多头"></a>为什么要使用多头</h3><p>根据下图多头注意力计算方式我们可以发现，在dm固定的情况下，不管是使用单头还是多头的方式，在实际的处理过程中直到进行注意力权重矩阵计算前，两者之前没有任何区别。当进行进行注意力权重矩阵计算时，ℎ越大那么Q，K，V就会被切分得越小，进而得到的注意力权重分配方式越多。图源：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/420820453">This post is all you need（上卷）——层层剥开Transformer</a></p>
<p><img src="https://s2.loli.net/2023/12/26/fPeLgozkDZTJUlB.png" alt=""></p>
<p><img src="https://s2.loli.net/2023/12/26/CqxhNcUBmvQfXtD.png" alt=""></p>
<p><img src="https://s2.loli.net/2023/12/26/fU8tBdcI7Hhr6Kw.png" alt=""></p>
<p><img src="https://s2.loli.net/2023/12/26/5mSsFQuTUnrpw4D.png" alt=""></p>
<p>从上图可以看出，如果ℎ=1，那么最终可能得到的就是一个各个位置只集中于自身位置的注意力权重矩阵；如果ℎ=2，那么就还可能得到另外一个注意力权重稍微分配合理的权重矩阵；ℎ=3同理如此。因而多头这一做法也恰好是论文作者提出用于克服<strong>模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置</strong>的问题。这里再插入一张真实场景下同一层的不同注意力权重矩阵可视化结果图：</p>
<p><img src="https://s2.loli.net/2023/12/26/Xa9j1fiK7TvHczk.png" alt=""></p>
<p>同时，当ℎ不一样时，dk的取值也不一样，进而使得对权重矩阵的scale的程度不一样。例如，如果dm=768，那么当ℎ=12时，则dk=64；当ℎ=1时，则dk=768。</p>
<p>所以，当模型的维度dm确定时，一定程度上ℎ越大整个模型的表达能力越强，越能提高模型对于注意力权重的合理分配。</p>
<h2 id="3-Encoder层"><a href="#3-Encoder层" class="headerlink" title="3.Encoder层"></a>3.Encoder层</h2><p>对于Encoder部分来说其内部主要由两部分网络所构成：多头注意力机制和两层前馈神经网络。同时，对于这两部分网络来说，都加入了残差连接，并且在残差连接后还进行了层归一化操作。这样，对于每个部分来说其输出均为<strong>LayerNorm(x+Sublayer(x))</strong>，并且在都加入了Dropout操作。对于第2部分的两层全连接网络来说，其具体计算过程为：</p>
<script type="math/tex; mode=display">
FFN(x) = max(0, xW1 + b1)W2 + b2</script><p><img src="https://s2.loli.net/2023/12/26/KSRYXptITNfEqbg.png" alt=""></p>
<h2 id="4-Decoder层"><a href="#4-Decoder层" class="headerlink" title="4.Decoder层"></a>4.Decoder层</h2><p>对于Decoder部分来说，其整体上与Encoder类似，只是多了一个用于与Encoder输出进行交互的多头注意力机制。</p>
<p><img src="https://s2.loli.net/2023/12/26/4EZbQwIN8zh3kJB.png" alt=""></p>
<p>不同于Encoder部分，在Decoder中一共包含有3个部分的网络结构。最上面的和最下面的部分（暂时忽略Mask）与Encoder相同，只是多了中间这个与Encoder输出（Memory）进行交互的部分，作者称之为“Encoder-Decoder attention”。对于这部分的输入，<strong>Q来自于下面多头注意力机制的输出，K和V均是Encoder部分的输出（Memory）经过线性变换后得到</strong>。</p>
<p>在Decoder对每一个时刻进行解码时，首先需要做的便是通过Q与 K进行交互（query查询），并计算得到注意力权重矩阵；然后再通过注意力权重与V进行计算得到一个权重向量，该权重向量所表示的含义就是在解码时如何将注意力分配到Memory的各个位置上。</p>
<h3 id="Decoder解码预测过程"><a href="#Decoder解码预测过程" class="headerlink" title="Decoder解码预测过程"></a>Decoder解码预测过程</h3><p><img src="https://s2.loli.net/2023/12/26/VmixW3Sj9N8tGou.gif" alt=""></p>
<p><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif" alt=""></p>
<h3 id="Decoder训练解码过程"><a href="#Decoder训练解码过程" class="headerlink" title="Decoder训练解码过程"></a>Decoder训练解码过程</h3><p>在介绍完预测时Decoder的解码过程后，下面就继续来看在网络在训练过程中是如何进行解码的。在真实预测时解码器需要将上一个时刻的输出作为下一个时刻解码的输入，然后一个时刻一个时刻的进行解码操作。显然，如果训练时也采用同样的方法那将是十分费时的。因此，在训练过程中，解码器也同编码器一样，一次接收解码时所有时刻的输入进行计算。这样做的好处，一是通过多样本并行计算能够加快网络的训练速度；二是在训练过程中直接喂入解码器正确的结果而不是上一时刻的预测值（因为训练时上一时刻的预测值可能是错误的）能够更好的训练网络。</p>
<p>例如在用平行预料<code>&quot;我 是 谁&quot;</code>&lt;==&gt;<code>&quot;who am i&quot;</code>对网络进行训练时，编码器的输入便是<code>&quot;我 是 谁&quot;</code>，而解码器的输入则是<code>&quot;&lt;s&gt; who am i&quot;</code>，对应的正确标签则是<code>&quot;who am i &lt;e&gt;&quot;</code></p>
<p>但是，模型在实际的预测过程中只是将当前时刻之前（包括当前时刻）的所有时刻作为输入来预测下一个时刻，也就是说模型在预测时是看不到当前时刻之后的信息。因此，Transformer中的Decoder通过加入注意力掩码机制来解决了这一问题。</p>
<p>左边依旧是通过Q和K计算得到了注意力权重矩阵（此时还未进行softmax操作），而中间的就是所谓的注意力掩码矩阵，两者在相加之后再乘上矩阵V便得到了整个自注意力机制的输出。</p>
<p><img src="https://s2.loli.net/2023/12/26/KY4AqnGalFO8ZBC.webp" alt=""></p>
<h2 id="5-残差和LayerNorm"><a href="#5-残差和LayerNorm" class="headerlink" title="5.残差和LayerNorm"></a>5.残差和LayerNorm</h2><h3 id="残差"><a href="#残差" class="headerlink" title="残差"></a>残差</h3><p>在Transformer中，数据过Attention层和FFN层后，都会经过一个<strong>Add &amp; Norm</strong>处理。其中，Add为<strong>residule block（残差模块）</strong>，数据在这里进行<strong>residule connection（残差连接）</strong>。</p>
<p>在transformer的encoder和decoder中，各用到了6层的attention模块，每一个attention模块又和一个FeedForward层（简称FFN）相接。对每一层的attention和FFN，都采用了一次残差连接，即把每一个位置的输入数据和输出数据相加，使得Transformer能够有效训练更深的网络。在残差连接过后，再采取Layer Nomalization的方式。具体的操作过程见下图：</p>
<p><img src="https://s2.loli.net/2023/12/26/UVRFyspOxIor48v.png" alt=""></p>
<h3 id="LayerNorm"><a href="#LayerNorm" class="headerlink" title="LayerNorm"></a>LayerNorm</h3><p><em>为什么在NLP任务中（Transformer），使用LayerNorm而不是BatchNorm。</em></p>
<p>首先，BN不适合RNN这种动态文本模型，有一个原因是因为batch中的长度不一致，导致有的靠后的特征的均值和方差不能估算。</p>
<p>但是这个问题不是大问题，可以在处理数据的适合，使句子的长度相近的在一个batch，所以这不是NLP中不用BN的核心原因。</p>
<p>BN是对每个特征的batch_size上求得均值和方差，就是对每个特征，这些特征都有明确得含义，比如身高、体重等。但是想象以下，如果BN应用到NLP任务，就变成了对每个单词，也就是默认了在同一个位置得单词对应的是同一个特征，比如：“我/期待/全新/AGI”和“今天/天气/真/不错“,使用BN代表着”我“和”今天“是对应同一个维度的特征，才能做BN。</p>
<p>LayNorm做的事针对每一个样本，做特征的缩放。也就是，它认为“我/期待/全新/AGI”这四个词在同一个特征下，基于此做归一化。这样做和BN的区别在于，一句话中的每个单词都可以归到这句话的”语义信息“这个特征中。所以这里不再对应batch_size，而是文本长度。</p>
<blockquote>
<p>引申-为什么BN在CNN中可以而在NLP中不行</p>
</blockquote>
<p>浅浅理解：BN在NLP中是对词向量进行缩放，词向量是我们学习出来的参数来表示词语语义的参数，不是真实存在的。而图像像素是真实存在的，像素中包含固有的信息。</p>
<h2 id="6-问题解析"><a href="#6-问题解析" class="headerlink" title="6.问题解析"></a>6.问题解析</h2><h3 id="Transformer的并行化"><a href="#Transformer的并行化" class="headerlink" title="Transformer的并行化"></a>Transformer的并行化</h3><p>Decoder不用多说，没有并行，只能一个一个解码，很类似于RNN，这个时刻的输入依赖于上一个时刻的输出。</p>
<p>Encoder：首先，6个大的模块之间是串行的，一个模块的计算结果作为下一个模块的输入，互相之间有依赖关系。对于每个模块，注意力层和前馈神经网络这两个子模块单独看都是可以并行的，不同的单词之间是没有依赖关系的，当然对于注意力层在做attention时会依赖别的时刻的输入。然后注意力层和前馈网络之间时串行的，必须先完成注意力层计算再做前馈网络。</p>
<h3 id="为什么Q和K使用不同的权重矩阵生成，不能使用同一个值进行自身的点乘"><a href="#为什么Q和K使用不同的权重矩阵生成，不能使用同一个值进行自身的点乘" class="headerlink" title="为什么Q和K使用不同的权重矩阵生成，不能使用同一个值进行自身的点乘"></a>为什么Q和K使用不同的权重矩阵生成，不能使用同一个值进行自身的点乘</h3><p>简单回答：使用QKV不相同可以保证在不同空间进行投影，增强了表达能力，提高泛化能力。</p>
<p>详细解释：</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/319339652">transformer中为什么使用不同的K 和 Q， 为什么不能使用同一个值？ - 知乎</a></p>
<h3 id="Transformer计算attention时候为什么选择点乘而不是加法，在计算复杂度和效果上有什么区别"><a href="#Transformer计算attention时候为什么选择点乘而不是加法，在计算复杂度和效果上有什么区别" class="headerlink" title="Transformer计算attention时候为什么选择点乘而不是加法，在计算复杂度和效果上有什么区别"></a>Transformer计算attention时候为什么选择点乘而不是加法，在计算复杂度和效果上有什么区别</h3><p>实验分析，时间复杂度相似；效果上，和dk相关，dk越大，加法效果越显著。</p>
<p><img src="https://s2.loli.net/2023/12/26/afey89HQ3WYlhbc.png" alt=""></p>
<h3 id="transformer中的attention为什么scaled"><a href="#transformer中的attention为什么scaled" class="headerlink" title="transformer中的attention为什么scaled"></a>transformer中的attention为什么scaled</h3><p>根据上一个问题，在dk（即 attention-dim）较小的时候，两者的效果接近。但是随着dk增大，Add 开始显著超越 Mul。作者分析 Mul 性能不佳的原因，认为是<strong>极大的点积值将整个 softmax 推向梯度平缓区，使得收敛困难</strong>。</p>
<p>这才有了 scaled。所以，Add 是天然地不需要 scaled，Mul 在dk较大的时候必须要做 scaled。个人认为，<strong>Add 中的矩阵乘法，和 Mul 中的矩阵乘法不同</strong>。前者中只有随机变量 X 和参数矩阵W 相乘，但是后者中包含随机变量 X 和随机变量 X 之间的乘法。</p>
<p>下面附上在知乎看到的代码数值实验就很清楚：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> softmax</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_gradient</span>(<span class="params">dim, time_steps=<span class="number">50</span>, scale=<span class="number">1.0</span></span>):</span><br><span class="line">    <span class="comment"># Assume components of the query and keys are drawn from N(0, 1) independently</span></span><br><span class="line">    q = np.random.randn(dim)</span><br><span class="line">    ks = np.random.randn(time_steps, dim)</span><br><span class="line">    x = np.<span class="built_in">sum</span>(q * ks, axis=<span class="number">1</span>) / scale  <span class="comment"># x.shape = (time_steps,) </span></span><br><span class="line">    y = softmax(x)</span><br><span class="line">    grad = np.diag(y) - np.outer(y, y)</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">max</span>(np.<span class="built_in">abs</span>(grad))  <span class="comment"># the maximum component of gradients</span></span><br><span class="line"></span><br><span class="line">NUMBER_OF_EXPERIMENTS = <span class="number">5</span></span><br><span class="line"><span class="comment"># results of 5 random runs without scaling</span></span><br><span class="line"><span class="built_in">print</span>([test_gradient(<span class="number">100</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(NUMBER_OF_EXPERIMENTS)])</span><br><span class="line"><span class="built_in">print</span>([test_gradient(<span class="number">1000</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(NUMBER_OF_EXPERIMENTS)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># results of 5 random runs with scaling</span></span><br><span class="line"><span class="built_in">print</span>([test_gradient(<span class="number">100</span>, scale=np.sqrt(<span class="number">100</span>)) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(NUMBER_OF_EXPERIMENTS)])</span><br><span class="line"><span class="built_in">print</span>([test_gradient(<span class="number">1000</span>, scale=np.sqrt(<span class="number">1000</span>)) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(NUMBER_OF_EXPERIMENTS)])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不带 scaling 的两组（可以看到 dim=1000 时很容易发生梯度消失）：</span></span><br><span class="line">[<span class="number">1.123056543761436e-06</span>, <span class="number">0.14117211040878508</span>, <span class="number">0.24896212285554725</span>, <span class="number">0.000861296669097178</span>, <span class="number">0.24717750591081689</span>]</span><br><span class="line">[<span class="number">1.2388028380883043e-09</span>, <span class="number">3.630249703743676e-18</span>, <span class="number">1.4210854715202004e-14</span>, <span class="number">1.1652900866465643e-12</span>, <span class="number">5.045026175709566e-07</span>]</span><br><span class="line"><span class="comment"># 带 scaling 的两组（dim=1000 时梯度流依然稳定）：</span></span><br><span class="line">[<span class="number">0.11292476415310426</span>, <span class="number">0.08650727907448993</span>, <span class="number">0.11307320939056421</span>, <span class="number">0.2039260641245917</span>, <span class="number">0.11422935801305531</span>]</span><br><span class="line">[<span class="number">0.12367058336991178</span>, <span class="number">0.15990586501130605</span>, <span class="number">0.1701746604359328</span>, <span class="number">0.10761820500367032</span>, <span class="number">0.07591586878293968</span>]</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://www.zhanghan.xyz">Zhang Han</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://www.zhanghan.xyz/posts/17281/">https://www.zhanghan.xyz/posts/17281/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.zhanghan.xyz" target="_blank">ZhangHan个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ChatGPT/">ChatGPT</a><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2023/12/26/gab76QJTiDO2BkC.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://s2.loli.net/2023/12/29/SlfdQGJnMtbiC3F.png" target="_blank"><img class="post-qr-code-img" src="https://s2.loli.net/2023/12/29/SlfdQGJnMtbiC3F.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://s2.loli.net/2023/12/29/nd7zrmV8hNs2a5i.jpg" target="_blank"><img class="post-qr-code-img" src="https://s2.loli.net/2023/12/29/nd7zrmV8hNs2a5i.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/231228/" title="GPT的现状、如何训练"><img class="cover" src="https://s2.loli.net/2023/12/28/ijmU2yPdSqAvY6L.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">GPT的现状、如何训练</div></div></a></div><div class="next-post pull-right"><a href="/posts/600227/" title="读《这就是ChatGPT》随记"><img class="cover" src="https://s2.loli.net/2023/12/21/uBKJbAsCeF9QnY1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">读《这就是ChatGPT》随记</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/20240117/" title="AIGC彩妆粉底设计"><img class="cover" src="https://s2.loli.net/2024/01/17/NZq9ohi4mVPTrWE.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-17</div><div class="title">AIGC彩妆粉底设计</div></div></a></div><div><a href="/posts/231228/" title="GPT的现状、如何训练"><img class="cover" src="https://s2.loli.net/2023/12/28/ijmU2yPdSqAvY6L.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-28</div><div class="title">GPT的现状、如何训练</div></div></a></div><div><a href="/posts/20240111/" title="ChatGPT账号快速注册"><img class="cover" src="https://s2.loli.net/2024/01/13/IaxVq7JwSMHGAKi.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-11</div><div class="title">ChatGPT账号快速注册</div></div></a></div><div><a href="/posts/231229/" title="Hexo+butterfly为文章添加AI摘要"><img class="cover" src="https://s2.loli.net/2023/12/29/clKfGeuDFiRzOP5.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-29</div><div class="title">Hexo+butterfly为文章添加AI摘要</div></div></a></div><div><a href="/posts/20240112/" title="一分钟快速搭建ChatGPT套壳网站"><img class="cover" src="https://s2.loli.net/2024/01/13/aVRmzcxk8fglnoe.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-12</div><div class="title">一分钟快速搭建ChatGPT套壳网站</div></div></a></div><div><a href="/posts/600227/" title="读《这就是ChatGPT》随记"><img class="cover" src="https://s2.loli.net/2023/12/21/uBKJbAsCeF9QnY1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-21</div><div class="title">读《这就是ChatGPT》随记</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9B%E5%85%A5Transformer"><span class="toc-number">1.</span> <span class="toc-text">进入Transformer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">Transformer是如何工作的？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%88Positional-Encodings%EF%BC%89"><span class="toc-number">2.1.</span> <span class="toc-text">1.位置编码（Positional Encodings）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Attention%EF%BC%89"><span class="toc-number">2.2.</span> <span class="toc-text">2.注意力机制（Attention）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Self-Attention%EF%BC%89"><span class="toc-number">2.3.</span> <span class="toc-text">3.自注意力机制（Self-Attention）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90"><span class="toc-number"></span> <span class="toc-text">Transformer技术解析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81Positional-Encoding"><span class="toc-number">1.</span> <span class="toc-text">1.位置编码Positional Encoding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">2.</span> <span class="toc-text">2.多头注意力机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#self-Attention%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">2.1.</span> <span class="toc-text">self-Attention自注意力机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MultiHeadAttention"><span class="toc-number">2.2.</span> <span class="toc-text">MultiHeadAttention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8%E5%A4%9A%E5%A4%B4"><span class="toc-number">2.3.</span> <span class="toc-text">为什么要使用多头</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Encoder%E5%B1%82"><span class="toc-number">3.</span> <span class="toc-text">3.Encoder层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Decoder%E5%B1%82"><span class="toc-number">4.</span> <span class="toc-text">4.Decoder层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder%E8%A7%A3%E7%A0%81%E9%A2%84%E6%B5%8B%E8%BF%87%E7%A8%8B"><span class="toc-number">4.1.</span> <span class="toc-text">Decoder解码预测过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder%E8%AE%AD%E7%BB%83%E8%A7%A3%E7%A0%81%E8%BF%87%E7%A8%8B"><span class="toc-number">4.2.</span> <span class="toc-text">Decoder训练解码过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%AE%8B%E5%B7%AE%E5%92%8CLayerNorm"><span class="toc-number">5.</span> <span class="toc-text">5.残差和LayerNorm</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE"><span class="toc-number">5.1.</span> <span class="toc-text">残差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LayerNorm"><span class="toc-number">5.2.</span> <span class="toc-text">LayerNorm</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E9%97%AE%E9%A2%98%E8%A7%A3%E6%9E%90"><span class="toc-number">6.</span> <span class="toc-text">6.问题解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer%E7%9A%84%E5%B9%B6%E8%A1%8C%E5%8C%96"><span class="toc-number">6.1.</span> <span class="toc-text">Transformer的并行化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88Q%E5%92%8CK%E4%BD%BF%E7%94%A8%E4%B8%8D%E5%90%8C%E7%9A%84%E6%9D%83%E9%87%8D%E7%9F%A9%E9%98%B5%E7%94%9F%E6%88%90%EF%BC%8C%E4%B8%8D%E8%83%BD%E4%BD%BF%E7%94%A8%E5%90%8C%E4%B8%80%E4%B8%AA%E5%80%BC%E8%BF%9B%E8%A1%8C%E8%87%AA%E8%BA%AB%E7%9A%84%E7%82%B9%E4%B9%98"><span class="toc-number">6.2.</span> <span class="toc-text">为什么Q和K使用不同的权重矩阵生成，不能使用同一个值进行自身的点乘</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer%E8%AE%A1%E7%AE%97attention%E6%97%B6%E5%80%99%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E6%8B%A9%E7%82%B9%E4%B9%98%E8%80%8C%E4%B8%8D%E6%98%AF%E5%8A%A0%E6%B3%95%EF%BC%8C%E5%9C%A8%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%92%8C%E6%95%88%E6%9E%9C%E4%B8%8A%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB"><span class="toc-number">6.3.</span> <span class="toc-text">Transformer计算attention时候为什么选择点乘而不是加法，在计算复杂度和效果上有什么区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transformer%E4%B8%AD%E7%9A%84attention%E4%B8%BA%E4%BB%80%E4%B9%88scaled"><span class="toc-number">6.4.</span> <span class="toc-text">transformer中的attention为什么scaled</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By Zhang Han</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a href="/隐私政策/">隐私政策</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'wuiUoLymCkLAFy3G045hw4Ua-gzGzoHsz',
      appKey: '3sBi4GBTzCgF7bwei4BOg1Q7',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadDisqus () {
  const disqus_config = function () {
    this.page.url = 'https://www.zhanghan.xyz/posts/17281/'
    this.page.identifier = '/posts/17281/'
    this.page.title = '解析Transformer模型'
  }

  const disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  btf.addModeChange('disqus', disqusReset)

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }
}

if ('Valine' === 'Disqus' || !true) {
  if (true) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script></div><div class="aplayer no-destroy" data-id="2625940695" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="false" muted></div><script src="/js/my.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="true" data-text="富强,民主,文明,和谐,平等,公正,法治,爱国,敬业,诚信,友善" data-fontsize="15px" data-random="true" async="async"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>